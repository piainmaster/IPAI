{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84c9c726c8d5dc34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.stats import entropy\n",
    "#from skimage.measure import shannon_entropy\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import LeaveOneGroupOut"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18bcdb0a3b7b46e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Methods for data import\n",
    "Methods loadPLUS(), loadSCUT() and loadVERA()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6774b9bd962be5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "method = cv2.INTER_LANCZOS4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "297c6174b3b45e87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loadPLUS():\n",
    "    # Define the path to the dataset\n",
    "    datasource_path = \"dataset/PLUS\"\n",
    "\n",
    "    data_PLUS_genuine, data_PLUS_spoofed, data_PLUS_003, data_PLUS_004 = [], [], [], []\n",
    "\n",
    "    # load dataset PLUS\n",
    "    p = Path(datasource_path + \"/\" + \"genuine\")\n",
    "    for filename in p.glob('**/*.png'):\n",
    "        _, tail = str(filename).rsplit(\"Laser_PALMAR_\", 1)\n",
    "        id = tail.split('_', 1)[0]\n",
    "        img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (736, 192), interpolation=method)\n",
    "        hist = histBin(img, bin)\n",
    "        data_PLUS_genuine.append([\"genuine\", img, hist, id])\n",
    "\n",
    "    p = Path(datasource_path + \"/\" + \"spoofed\")\n",
    "    for filename in p.glob('**/*.png'):\n",
    "        _, tail = str(filename).rsplit(\"Laser_PALMAR_\", 1)\n",
    "        id = tail.split('_', 1)[0]\n",
    "        img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (736, 192), interpolation=method)\n",
    "        histBin(img, bin)\n",
    "        data_PLUS_spoofed.append([\"spoofed\", img, hist, id])\n",
    "\n",
    "    for synthethic_category in [\"spoofed_synthethic_cyclegan\",\n",
    "                                \"spoofed_synthethic_distancegan\",\n",
    "                                \"spoofed_synthethic_drit\",\n",
    "                                \"spoofed_synthethic_stargan-v2\"]:\n",
    "        for variant in [\"003\"]:\n",
    "            for fold in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n",
    "                p = Path(datasource_path + \"/\" + synthethic_category + \"/\" + variant + \"/\" + fold)\n",
    "                for filename in p.glob('**/*.png'):\n",
    "                    _, tail = str(filename).rsplit(\"Laser_PALMAR_\", 1)\n",
    "                    id = tail.split('_', 1)[0]\n",
    "                    img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "                    img = cv2.resize(img, (736, 192), interpolation=method)\n",
    "                    hist = histBin(img, bin)\n",
    "                    data_PLUS_003.append([synthethic_category, img, hist, id])\n",
    "        for variant in [\"004\"]:\n",
    "            for fold in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n",
    "                p = Path(datasource_path + \"/\" + synthethic_category + \"/\" + variant + \"/\" + fold)\n",
    "                for filename in p.glob('**/*.png'):\n",
    "                    _, tail = str(filename).rsplit(\"Laser_PALMAR_\", 1)\n",
    "                    id = tail.split('_', 1)[0]\n",
    "                    img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "                    img = cv2.resize(img, (736, 192), interpolation=method)\n",
    "                    hist = histBin(img, bin)\n",
    "                    data_PLUS_004.append([synthethic_category, img, hist, id])\n",
    "\n",
    "    return data_PLUS_genuine, data_PLUS_spoofed, data_PLUS_003, data_PLUS_004"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a56114be7323058a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loadSCUT():\n",
    "    # Define the path to the dataset\n",
    "    datasource_path = \"dataset/SCUT\"\n",
    "\n",
    "    data_SCUT_genuine, data_SCUT_spoofed, data_SCUT_007, data_SCUT_008 = [], [], [], []\n",
    "\n",
    "    # load dataset SCUT\n",
    "    for id in [\"001\", \"005\", \"009\", \"013\", \"017\", \"021\", \"025\", \"029\", \"033\", \"037\",\n",
    "                    \"041\", \"045\", \"049\", \"053\", \"057\", \"061\", \"065\", \"069\"]:\n",
    "        p = Path(datasource_path + \"/\" + \"genuine\" + \"/\" + id)\n",
    "        for filename in p.glob('**/*.bmp'):\n",
    "            img = cv2.rotate(cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE), cv2.ROTATE_90_CLOCKWISE)\n",
    "            img = cv2.resize(img, (639, 287), interpolation=method)\n",
    "            hist = histBin(img, bin)\n",
    "            data_SCUT_genuine.append([\"genuine\", img, hist, id])\n",
    "\n",
    "    for id in [\"001\", \"005\", \"009\", \"013\", \"017\", \"021\", \"025\", \"029\", \"033\", \"037\",\n",
    "                    \"041\", \"045\", \"049\", \"053\", \"057\", \"061\", \"065\", \"069\"]:\n",
    "        p = Path(datasource_path + \"/\" + \"spoofed\" + \"/\" + id)\n",
    "        for filename in p.glob('**/*.bmp'):\n",
    "            img = cv2.rotate(cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE), cv2.ROTATE_90_CLOCKWISE)\n",
    "            img = cv2.resize(img, (639, 287), interpolation=method)\n",
    "            hist = histBin(img, bin)\n",
    "            data_SCUT_spoofed.append([\"spoofed\", img, hist, id])\n",
    "\n",
    "    for synthethic_category in [\"spoofed_synthethic_cyclegan\",\n",
    "                                \"spoofed_synthethic_distancegan\",\n",
    "                                \"spoofed_synthethic_drit\",\n",
    "                                \"spoofed_synthethic_stargan-v2\"]:\n",
    "        for variant in [\"007\"]:\n",
    "            for fold in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n",
    "                p = Path(datasource_path + \"/\" + synthethic_category + \"/\" + variant + \"/\" + fold + \"/reference\")\n",
    "                for filename in p.glob('**/*.png'):\n",
    "                    _, tail = os.path.split(filename)\n",
    "                    id = tail.split('-', 1)[0]\n",
    "                    if id in [\"001\", \"005\", \"009\", \"013\", \"017\", \"021\", \"025\", \"029\", \"033\", \"037\",\n",
    "                    \"041\", \"045\", \"049\", \"053\", \"057\", \"061\", \"065\", \"069\"]:\n",
    "                        img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (639, 287), interpolation=method)\n",
    "                        hist = histBin(img, bin)\n",
    "                        hist = histBin(img, bin)  \n",
    "                        data_SCUT_007.append([synthethic_category, img, hist, id])\n",
    "        for variant in [\"008\"]:\n",
    "            for fold in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n",
    "                p = Path(datasource_path + \"/\" + synthethic_category + \"/\" + variant + \"/\" + fold + \"/reference\")\n",
    "                for filename in p.glob('**/*.png'):\n",
    "                    _, tail = os.path.split(filename)\n",
    "                    id = tail.split('-', 1)[0]\n",
    "                    if id in [\"001\", \"005\", \"009\", \"013\", \"017\", \"021\", \"025\", \"029\", \"033\", \"037\",\n",
    "                                   \"041\", \"045\", \"049\", \"053\", \"057\", \"061\", \"065\", \"069\"]:\n",
    "                        img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (639, 287), interpolation=method)\n",
    "                        hist = histBin(img, bin)\n",
    "                        data_SCUT_008.append([synthethic_category, img, hist, id])\n",
    "\n",
    "    return data_SCUT_genuine, data_SCUT_spoofed, data_SCUT_007, data_SCUT_008"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef84e83398f9125b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loadVERA():\n",
    "    # Define the path to the dataset\n",
    "    datasource_path = \"dataset/IDIAP\"\n",
    "\n",
    "    data_VERA_genuine, data_VERA_spoofed, data_VERA_009 = [], [], []\n",
    "\n",
    "    # load dataset PLUS\n",
    "    p = Path(datasource_path + \"/\" + \"genuine\")\n",
    "    for filename in p.glob('**/*.png'):\n",
    "        _, tail = os.path.split(filename)\n",
    "        id = tail.split('_', 1)[0]      #todo: only consider images from 001-109 (not -113)\n",
    "        img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (664, 248), interpolation=method)\n",
    "        hist = histBin(img, bin)\n",
    "        data_VERA_genuine.append([\"genuine\", img, hist, id])\n",
    "\n",
    "    p = Path(datasource_path + \"/\" + \"spoofed\")\n",
    "    for filename in p.glob('**/*.png'):\n",
    "        _, tail = os.path.split(filename)\n",
    "        id = tail.split('_', 1)[0]  # todo: only consider images from 001-109 (not -113)\n",
    "        img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (664, 248), interpolation=method)\n",
    "        hist = histBin(img, bin)\n",
    "        data_VERA_spoofed.append([\"spoofed\", img, hist, id])\n",
    "\n",
    "    for synthethic_category in [\"spoofed_synthethic_cyclegan\",\n",
    "                                \"spoofed_synthethic_distancegan\",\n",
    "                                \"spoofed_synthethic_drit\",\n",
    "                                \"spoofed_synthethic_stargan-v2\"]:\n",
    "        for variant in [\"009\"]:\n",
    "            for fold in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n",
    "                p = Path(datasource_path + \"/\" + synthethic_category + \"/\" + variant + \"/\" + fold)\n",
    "                for filename in p.glob('**/*.png'):\n",
    "                    _, tail = os.path.split(filename)\n",
    "                    tail_front = tail.split('_', 1)[0]\n",
    "                    id = tail_front.split('-', 1)[1]\n",
    "                    img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "                    img = cv2.resize(img, (664, 248), interpolation=method)\n",
    "                    hist = histBin(img, bin)\n",
    "                    data_VERA_009.append([synthethic_category, img, hist, id])\n",
    "                \n",
    "                for filename in p.glob('**/*.jpg'):\n",
    "                    _, tail = os.path.split(filename)\n",
    "                    tail_front = tail.split('_', 1)[0]\n",
    "                    id = tail_front.split('-', 1)[1]\n",
    "                    img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "                    img = cv2.resize(img, (664, 248), interpolation=method)\n",
    "                    hist = histBin(img, bin)\n",
    "                    data_VERA_009.append([synthethic_category, img, hist, id])\n",
    "\n",
    "    return data_VERA_genuine, data_VERA_spoofed, data_VERA_009"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8891ef327f99019"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Methods\n",
    "For knn and feature calculation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c29d4e1385ce99af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from skimage.measure import shannon_entropy\n",
    "import scipy\n",
    "\n",
    "\n",
    "def combine_list_with_genuine(list):\n",
    "    current_data = data_genuine + list\n",
    "    return current_data\n",
    "    \n",
    "# -----------------------------------------------------------------------\n",
    "# KNN\n",
    "# -----------------------------------------------------------------------\n",
    "def knncalc(k, feature_list):\n",
    "    knn_list = list()\n",
    "    feature_list.sort(key=lambda tup: tup[1]) #nach den Werten in der zweiten Spalte sortieren (kng)\n",
    "    for i in range(k):\n",
    "        knn_list.append(feature_list[i][0]) # es werden \"spoofed\" und \"genuine\" eingetragen\n",
    "    fdist = dict(zip(*np.unique(knn_list, return_counts=True)))\n",
    "    pred = max(fdist, key=fdist.get)\n",
    "    return pred\n",
    "\n",
    "\n",
    "####### for last step if we have several feature classifications\n",
    "def most_common_value(list):\n",
    "    mcv = dict(zip(*np.unique(list, return_counts=True)))\n",
    "    pred = list(mcv)[-1]\n",
    "    return pred\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# HISTOGRAM\n",
    "# -----------------------------------------------------------------------\n",
    "def histBin(img, bins):\n",
    "    if bins < 257:\n",
    "        hist = cv2.calcHist([img], [0], None, [bin], [0, 256])\n",
    "        return hist\n",
    "    else:\n",
    "        return \"Falscher Bin-Wert\"\n",
    "\n",
    "\n",
    "def intersection_test(hist1, hist2):\n",
    "    value = np.sum(np.minimum(hist1, hist2))\n",
    "    return -value\n",
    "\n",
    "\n",
    "def euclidean_distance_test(hist1, hist2):\n",
    "    sum = 0\n",
    "    for i in range(0, bin):\n",
    "        sum = sum + (hist1[i][0] - hist2[0][i][0]) ** 2\n",
    "    eu_dist = math.sqrt(sum)\n",
    "    return eu_dist\n",
    "\n",
    "\n",
    "def sum_manhattan_distance_test(hist1, hist2):\n",
    "    sum = 0\n",
    "    for i in range(0, bin):\n",
    "        sum = sum + abs(hist1[i][0] - hist2[0][i][0])\n",
    "    ma_dist = sum\n",
    "    return ma_dist\n",
    "\n",
    "\n",
    "# earthmovers distance\n",
    "def em_dist(img1, img2):\n",
    "    images = [img.ravel() for img in [img1, img2]]\n",
    "    em_distance = wasserstein_distance(images[0], images[1])\n",
    "    return em_distance\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# ENTROPY\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# calculate Entropy with frames--> takes nr of frames and image path and gives back an array of entropies:\n",
    "def calculate_entropies(image, num_frames):\n",
    "    # Divide the image into frames\n",
    "    entropies = []\n",
    "    size = image.shape\n",
    "    height, width = size[0], size[1]\n",
    "\n",
    "    frame_size = width // num_frames\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        start_x = i * frame_size\n",
    "        end_x = (i + 1) * frame_size\n",
    "        frame = image[:, start_x:end_x]\n",
    "        entropy = shannon_entropy(frame)\n",
    "        entropies.append(entropy)\n",
    "\n",
    "    return entropies\n",
    "\n",
    "# Function to calculate patch entropies of an image with size patch_x x patch_y with no overlap between the patches\n",
    "def calculate_patch_entropies(image, patch_x, patch_y):\n",
    "    patch_size = (patch_x, patch_y)\n",
    "    entropies = []\n",
    "    h, w = image.shape\n",
    "    for i in range(0, h, patch_size[0]):\n",
    "        for j in range(0, w, patch_size[1]):\n",
    "            patch = image[i:i+patch_size[0], j:j+patch_size[1]]\n",
    "            entropies.append(shannon_entropy(patch))\n",
    "    return entropies\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# VARIANCE\n",
    "# -----------------------------------------------------------------------\n",
    "def mean_absolute_difference(a, b):\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"Input lists must have the same length\")\n",
    "    \n",
    "    return sum(abs(a_i - b_i) for a_i, b_i in zip(a, b)) / len(a)\n",
    "\n",
    "def calculate_variances(image, num_frames):\n",
    "    # Divide the image into frames\n",
    "    variances = []\n",
    "    size = image.shape\n",
    "    height, width = size[0], size[1]\n",
    "\n",
    "    frame_size = width // num_frames\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        start_x = i * frame_size\n",
    "        end_x = (i + 1) * frame_size\n",
    "        frame = image[:, start_x:end_x]\n",
    "        variance = np.var(frame)\n",
    "        variances.append(variance)\n",
    "\n",
    "    return variances\n",
    "\n",
    "\n",
    "# Function to calculate patch variances of an image with size patch_x x patch_y with no overlap between the patches\n",
    "def calculate_patch_variances(image, patch_x, patch_y):\n",
    "    patch_size = (patch_x, patch_y)\n",
    "    variances = []\n",
    "    h, w = image.shape\n",
    "    for i in range(0, h, patch_size[0]):\n",
    "        for j in range(0, w, patch_size[1]):\n",
    "            patch = image[i:i+patch_size[0], j:j+patch_size[1]]\n",
    "            variances.append(np.var(patch))\n",
    "    return variances"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bf6bdd1305afa0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data import\n",
    "\n",
    "Please choose the dataset (PLUS, VERA, SCUT) to be analysed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f9e9f7c87e888d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# METHODEN-AUFRUFE\n",
    "# -----------------------------------------------------------------------\n",
    "method = cv2.INTER_LANCZOS4\n",
    "bin = 6\n",
    "\n",
    "#data_genuine, data_spoofed, data_PLUS_003, data_PLUS_004 = loadPLUS()\n",
    "data_genuine, data_spoofed, data_VERA_009 = loadVERA()\n",
    "#data_genuine, data_spoofed, data_SCUT_007,  data_SCUT_008 = loadSCUT()\n",
    "\n",
    "# to be changed:\n",
    "generation_variant = data_VERA_009\n",
    "generation_method = \"spoofed_synthethic_cyclegan\" #\"spoofed_synthethic_distancegan\"  #\"spoofed_synthethic_distancegan\"#\"spoofed_synthethic_drit\"  #\"spoofed_synthethic_stargan-v2\"\n",
    "\n",
    "data_synthetic = []\n",
    "for row in generation_variant:\n",
    "    if row[0] == generation_method:\n",
    "        row[0] = \"synthetic\"\n",
    "        data_synthetic.append(row)\n",
    "\n",
    "if len(data_synthetic) < len(data_genuine):\n",
    "    data_genuine = data_genuine[:(len(data_synthetic))]\n",
    "    data_spoofed = data_spoofed[:(len(data_synthetic))]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfa2422ecae5dccc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(data_genuine)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7acc845a7aa2ee7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# CURRENT TRAIN DATA\n",
    "# -----------------------------------------------------------------------\n",
    "# combine genuine data with synthetic data \n",
    "current_data = []\n",
    "current_data = combine_list_with_genuine(data_synthetic)\n",
    "\n",
    "# convert data to numpy array\n",
    "labels_list, images_list, histograms_list, id_list = [], [], [], []\n",
    "for row in current_data:\n",
    "    labels_list.append(row[0])\n",
    "    images_list.append(row[1])\n",
    "    histograms_list.append(row[2])\n",
    "    id_list.append(row[3])\n",
    "\n",
    "labels = np.array(labels_list)\n",
    "images = np.array(images_list)\n",
    "histograms = np.array(histograms_list)\n",
    "id = np.array(id_list)\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# CURRENT TEST DATA\n",
    "# -----------------------------------------------------------------------\n",
    "# combine lists data_PLUS_genuine and data_PLUS_spoofed\n",
    "validation_data = []\n",
    "validation_data = combine_list_with_genuine(data_spoofed)         \n",
    "\n",
    "# convert data to numpy array\n",
    "validation_labels_list, validation_images_list, validation_histograms_list, validation_id_list = [], [], [], []\n",
    "for row in validation_data:\n",
    "    validation_labels_list.append(row[0])\n",
    "    validation_images_list.append(row[1])\n",
    "    validation_histograms_list.append(row[2])\n",
    "    validation_id_list.append(row[3])\n",
    "\n",
    "validation_labels = np.array(validation_labels_list)\n",
    "validation_images = np.array(validation_images_list)\n",
    "validation_histograms = np.array(validation_histograms_list)\n",
    "validation_id = np.array(validation_id_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66a83f131393bfa8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Calculation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b2856f7738bd78a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()\n",
    "pred_list = []"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78eec949179c73e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Entropy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9d51d6becd931bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# CALCULATE FEATURE entropy\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# calculate feature entropy\n",
    "entropy_list1 = []\n",
    "entropy_list10 = []\n",
    "entropy_list20 = []\n",
    "entropy_list30 = []\n",
    "entropy_list40 = []\n",
    "entropy_list50 = []\n",
    "entropy_list100 = []\n",
    "\n",
    "#note: these patches divide image in vertical stripes\n",
    "for img in range(len(images)):\n",
    "    #global entropy\n",
    "    entropies1 = calculate_entropies(images[img], 1)\n",
    "    entropy_list1.append([labels[img], entropies1])\n",
    "    #entropy with 10 patches\n",
    "    entropies10 = calculate_entropies(images[img], 10)\n",
    "    entropy_list10.append([labels[img], entropies10])\n",
    "    #entropy with 20 patches\n",
    "    entropies20 = calculate_entropies(images[img], 20)\n",
    "    entropy_list20.append([labels[img], entropies20])\n",
    "    #entropy with 30 patches\n",
    "    entropies30 = calculate_entropies(images[img], 30)\n",
    "    entropy_list30.append([labels[img], entropies30])\n",
    "    #entropy with 40 patches\n",
    "    entropies40 = calculate_entropies(images[img], 40)\n",
    "    entropy_list40.append([labels[img], entropies40])\n",
    "    #entropy with 50 patches\n",
    "    entropies50 = calculate_entropies(images[img], 50)\n",
    "    entropy_list50.append([labels[img], entropies50])\n",
    "    #entropy with 100 patches\n",
    "    entropies100 = calculate_entropies(images[img], 100)\n",
    "    entropy_list100.append([labels[img], entropies100])\n",
    "\n",
    "\n",
    "\n",
    "entropy_list10x10 = []\n",
    "entropy_list20x20 = []\n",
    "entropy_list30x30 = []\n",
    "entropy_list40x40 = []\n",
    "entropy_list50x50 = []\n",
    "entropy_list70x70 = []\n",
    "entropy_list100x100 = []\n",
    "entropy_list100x200 = []\n",
    "\n",
    "#note: these patches divide in a x b sized patches\n",
    "for img in range(len(images)):\n",
    "    #entropy with 10x10 patches\n",
    "    entropies10x10 = calculate_patch_entropies(images[img], 10, 10)\n",
    "    entropy_list10x10.append([labels[img], entropies10x10])\n",
    "    #entropy with 20x20 patches\n",
    "    entropies20x20 = calculate_patch_entropies(images[img], 20, 20)\n",
    "    entropy_list20x20.append([labels[img], entropies20x20])\n",
    "    #entropy with 30x30 patches\n",
    "    entropies30x30 = calculate_patch_entropies(images[img], 30, 30)\n",
    "    entropy_list30x30.append([labels[img], entropies30x30])\n",
    "    #entropy with 40x40 patches\n",
    "    entropies40x40 = calculate_patch_entropies(images[img], 40, 40)\n",
    "    entropy_list40x40.append([labels[img], entropies40x40])\n",
    "    #entropy with 50x50 patches\n",
    "    entropies50x50 = calculate_patch_entropies(images[img], 50, 50)\n",
    "    entropy_list50x50.append([labels[img], entropies50x50])\n",
    "    # entropy with 70x70 patches\n",
    "    entropies70x70 = calculate_patch_entropies(images[img], 70, 70)\n",
    "    entropy_list70x70.append([labels[img], entropies70x70])\n",
    "    #entropy with 100x100 patches\n",
    "    entropies100x100 = calculate_patch_entropies(images[img], 100, 100)\n",
    "    entropy_list100x100.append([labels[img], entropies100x100])\n",
    "    #entropy with 100x200 patches\n",
    "    entropies100x200 = calculate_patch_entropies(images[img], 100, 200)\n",
    "    entropy_list100x200.append([labels[img], entropies100x200])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6cf9df8eb37061c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Leave One Out"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19cbc2c564f16637"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for k_knn in [3, 5, 7, 9]:\n",
    "    \n",
    "    correct_entropy1_preds = 0\n",
    "    correct_entropy10_preds = 0\n",
    "    correct_entropy20_preds = 0\n",
    "    correct_entropy30_preds = 0\n",
    "    correct_entropy40_preds = 0\n",
    "    correct_entropy50_preds = 0\n",
    "    correct_entropy100_preds = 0    \n",
    "    correct_entropy10x10_preds = 0\n",
    "    correct_entropy20x20_preds = 0\n",
    "    correct_entropy30x30_preds = 0\n",
    "    correct_entropy40x40_preds = 0\n",
    "    correct_entropy50x50_preds = 0\n",
    "    correct_entropy70x70_preds = 0\n",
    "    correct_entropy100x100_preds = 0\n",
    "    correct_entropy100x200_preds = 0\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(loo.split(validation_images)):\n",
    "        current_id = validation_id[test_index]\n",
    "        \n",
    "        test_entropy1 = [validation_labels[test_index][0], calculate_entropies(validation_images[test_index], num_frames=1)]\n",
    "        entropy1_distances = []\n",
    "        test_entropy10 = [validation_labels[test_index][0], calculate_entropies(validation_images[test_index][0], 10)]\n",
    "        entropy10_distances = []\n",
    "        test_entropy20 = [validation_labels[test_index][0], calculate_entropies(validation_images[test_index][0], 20)]\n",
    "        entropy20_distances = []\n",
    "        test_entropy30 = [validation_labels[test_index][0], calculate_entropies(validation_images[test_index][0], 30)]\n",
    "        entropy30_distances = []\n",
    "        test_entropy40 = [validation_labels[test_index][0], calculate_entropies(validation_images[test_index][0], 40)]\n",
    "        entropy40_distances = []\n",
    "        test_entropy50 = [validation_labels[test_index][0], calculate_entropies(validation_images[test_index][0], 50)]\n",
    "        entropy50_distances = []\n",
    "        test_entropy100 = [validation_labels[test_index][0], calculate_entropies(validation_images[test_index][0], 100)]\n",
    "        entropy100_distances = []\n",
    "        \n",
    "        test_entropy10x10 = [validation_labels[test_index][0], calculate_patch_entropies(validation_images[test_index][0], 10, 10)]\n",
    "        entropy10x10_distances = []\n",
    "        test_entropy20x20 = [validation_labels[test_index][0], calculate_patch_entropies(validation_images[test_index][0], 20, 20)]\n",
    "        entropy20x20_distances = []\n",
    "        test_entropy30x30 = [validation_labels[test_index][0], calculate_patch_entropies(validation_images[test_index][0], 30, 30)]\n",
    "        entropy30x30_distances = []\n",
    "        test_entropy40x40 = [validation_labels[test_index][0], calculate_patch_entropies(validation_images[test_index][0], 40, 40)]\n",
    "        entropy40x40_distances = []\n",
    "        test_entropy50x50 = [validation_labels[test_index][0], calculate_patch_entropies(validation_images[test_index][0], 50, 50)]\n",
    "        entropy50x50_distances = []\n",
    "        test_entropy70x70 = [validation_labels[test_index][0], calculate_patch_entropies(validation_images[test_index][0], 70, 70)]\n",
    "        entropy70x70_distances = []\n",
    "        test_entropy100x100 = [validation_labels[test_index][0], calculate_patch_entropies(validation_images[test_index][0], 100, 100)]\n",
    "        entropy100x100_distances = []\n",
    "        test_entropy100x200 = [validation_labels[test_index][0], calculate_patch_entropies(validation_images[test_index][0], 100, 200)]\n",
    "        entropy100x200_distances = []\n",
    "    \n",
    "    \n",
    "        for j in range(len(labels)):\n",
    "            entropy1_distances.append([entropy_list1[j][0], mean_absolute_difference(entropy_list1[j][1], test_entropy1[1])])\n",
    "            entropy10_distances.append([entropy_list10[j][0], mean_absolute_difference(entropy_list10[j][1], test_entropy10[1])])\n",
    "            entropy20_distances.append([entropy_list20[j][0], mean_absolute_difference(entropy_list20[j][1], test_entropy20[1])])\n",
    "            entropy30_distances.append([entropy_list30[j][0], mean_absolute_difference(entropy_list30[j][1], test_entropy30[1])])\n",
    "            entropy40_distances.append([entropy_list40[j][0], mean_absolute_difference(entropy_list40[j][1], test_entropy40[1])])\n",
    "            entropy50_distances.append([entropy_list50[j][0], mean_absolute_difference(entropy_list50[j][1], test_entropy50[1])])\n",
    "            entropy100_distances.append([entropy_list100[j][0], mean_absolute_difference(entropy_list100[j][1], test_entropy100[1])])         \n",
    "        \n",
    "            entropy10x10_distances.append([entropy_list10x10[j][0], mean_absolute_difference(entropy_list10x10[j][1], test_entropy10x10[1])])\n",
    "            entropy20x20_distances.append([entropy_list20x20[j][0], mean_absolute_difference(entropy_list20x20[j][1], test_entropy20x20[1])])\n",
    "            entropy30x30_distances.append([entropy_list30x30[j][0], mean_absolute_difference(entropy_list30x30[j][1], test_entropy30x30[1])])\n",
    "            entropy40x40_distances.append([entropy_list40x40[j][0], mean_absolute_difference(entropy_list40x40[j][1], test_entropy40x40[1])])\n",
    "            entropy50x50_distances.append([entropy_list50x50[j][0], mean_absolute_difference(entropy_list50x50[j][1], test_entropy50x50[1])])\n",
    "            entropy70x70_distances.append([entropy_list70x70[j][0], mean_absolute_difference(entropy_list70x70[j][1], test_entropy70x70[1])])\n",
    "            entropy100x100_distances.append([entropy_list100x100[j][0], mean_absolute_difference(entropy_list100x100[j][1], test_entropy100x100[1])])\n",
    "            entropy100x200_distances.append([entropy_list100x200[j][0], mean_absolute_difference(entropy_list100x200[j][1], test_entropy100x200[1])])\n",
    "    \n",
    "    \n",
    "        # prediction for current test image\n",
    "        if (knncalc(k_knn, entropy1_distances) == 'genuine' and test_entropy1[0] == 'genuine') or (knncalc(k_knn, entropy1_distances) != 'genuine' and test_entropy1[0] != 'genuine'): correct_entropy1_preds += 1\n",
    "        if (knncalc(k_knn, entropy10_distances) == 'genuine' and test_entropy10[0] == 'genuine') or (knncalc(k_knn, entropy10_distances) != 'genuine' and test_entropy10[0] != 'genuine'): correct_entropy10_preds += 1\n",
    "        if (knncalc(k_knn, entropy20_distances) == 'genuine' and test_entropy20[0] == 'genuine') or (knncalc(k_knn, entropy20_distances) != 'genuine' and test_entropy20[0] != 'genuine'): correct_entropy20_preds += 1\n",
    "        if (knncalc(k_knn, entropy30_distances) == 'genuine' and test_entropy30[0] == 'genuine') or (knncalc(k_knn, entropy30_distances) != 'genuine' and test_entropy30[0] != 'genuine'): correct_entropy30_preds += 1\n",
    "        if knncalc(k_knn, entropy40_distances) == 'genuine' and test_entropy40[0] == 'genuine' or knncalc(k_knn, entropy40_distances) != 'genuine' and test_entropy40[0] != 'genuine': correct_entropy40_preds += 1\n",
    "        if knncalc(k_knn, entropy50_distances) == 'genuine' and test_entropy50[0] == 'genuine' or knncalc(k_knn, entropy50_distances) != 'genuine' and test_entropy50[0] != 'genuine': correct_entropy50_preds += 1\n",
    "        if knncalc(k_knn, entropy100_distances) == 'genuine' and test_entropy100[0] == 'genuine' or knncalc(k_knn, entropy100_distances) != 'genuine' and test_entropy100[0] != 'genuine': correct_entropy100_preds += 1\n",
    "        if (knncalc(k_knn, entropy10x10_distances) == 'genuine' and test_entropy10x10[0] == 'genuine') or (knncalc(k_knn, entropy10x10_distances) != 'genuine' and test_entropy10x10[0] != 'genuine'): correct_entropy10x10_preds += 1\n",
    "        if knncalc(k_knn, entropy20x20_distances) == 'genuine' and test_entropy20x20[0] == 'genuine' or knncalc(k_knn, entropy20x20_distances) != 'genuine' and test_entropy20x20[0] != 'genuine': correct_entropy20x20_preds += 1\n",
    "        if knncalc(k_knn, entropy30x30_distances) == 'genuine' and test_entropy30x30[0] == 'genuine' or knncalc(k_knn, entropy30x30_distances) != 'genuine' and test_entropy30x30[0] != 'genuine': correct_entropy30x30_preds += 1\n",
    "        if knncalc(k_knn, entropy40x40_distances) == 'genuine' and test_entropy40x40[0] == 'genuine' or knncalc(k_knn, entropy40x40_distances) != 'genuine' and test_entropy40x40[0] != 'genuine': correct_entropy40x40_preds += 1\n",
    "        if knncalc(k_knn, entropy50x50_distances) == 'genuine' and test_entropy50x50[0] == 'genuine' or knncalc(k_knn, entropy50x50_distances) != 'genuine' and test_entropy50x50[0] != 'genuine': correct_entropy50x50_preds += 1\n",
    "        if knncalc(k_knn, entropy70x70_distances) == 'genuine' and test_entropy70x70[0] == 'genuine' or knncalc(k_knn, entropy70x70_distances) != 'genuine' and test_entropy70x70[0] != 'genuine': correct_entropy70x70_preds += 1\n",
    "        if knncalc(k_knn, entropy100x100_distances) == 'genuine' and test_entropy100x100[0] == 'genuine' or knncalc(k_knn, entropy100x100_distances) != 'genuine' and test_entropy100x100[0] != 'genuine': correct_entropy100x100_preds += 1\n",
    "        if knncalc(k_knn, entropy100x200_distances) == 'genuine' and test_entropy100x200[0] == 'genuine' or knncalc(k_knn, entropy100x200_distances) != 'genuine' and test_entropy100x200[0] != 'genuine': correct_entropy100x200_preds += 1\n",
    "    \n",
    "        \n",
    "    # -----------------------------------------------------------------------\n",
    "    # OUTPUT RESULTS\n",
    "    # -----------------------------------------------------------------------\n",
    "    print(f'Leave One Out')\n",
    "    total = len(validation_labels)\n",
    "    print(f'Total number of samples: {str(total)}')\n",
    "    print(f'Classification results \\nentropies with knn {k_knn}')\n",
    "    \n",
    "    print(f'Accuracy global entropy: {correct_entropy1_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 10 patches: {correct_entropy10_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 20 patches: {correct_entropy20_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 30 patches: {correct_entropy30_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 40 patches: {correct_entropy40_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 50 patches: {correct_entropy50_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 100 patches: {correct_entropy100_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 10x10 patches: {correct_entropy10x10_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 20x20 patches: {correct_entropy20x20_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 30x30 patches: {correct_entropy30x30_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 40x40 patches: {correct_entropy40x40_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 50x50 patches: {correct_entropy50x50_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 70x70 patches: {correct_entropy70x70_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 100x100 patches: {correct_entropy100x100_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 100x200 patches: {correct_entropy100x200_preds / total * 100:.3f}%')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f26927da67c38867"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Leave One Subject Out"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0db3897e50be108"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for k_knn in [3, 5, 7, 9]:\n",
    "    \n",
    "    correct_entropy1_preds = 0\n",
    "    correct_entropy10_preds = 0\n",
    "    correct_entropy20_preds = 0\n",
    "    correct_entropy30_preds = 0\n",
    "    correct_entropy40_preds = 0\n",
    "    correct_entropy50_preds = 0\n",
    "    correct_entropy100_preds = 0\n",
    "    correct_entropy10x10_preds = 0\n",
    "    correct_entropy20x20_preds = 0\n",
    "    correct_entropy30x30_preds = 0\n",
    "    correct_entropy40x40_preds = 0\n",
    "    correct_entropy50x50_preds = 0\n",
    "    correct_entropy70x70_preds = 0\n",
    "    correct_entropy100x100_preds = 0\n",
    "    correct_entropy100x200_preds = 0\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(loo.split(validation_images)):\n",
    "        current_id = validation_id[test_index]\n",
    "        \n",
    "        test_entropy1 = [validation_labels[test_index][0], calculate_entropies(validation_images[test_index], num_frames=1)]\n",
    "        entropy1_distances = []\n",
    "        test_entropy10 = [validation_labels[test_index][0], calculate_entropies(validation_images[test_index], num_frames=10)]\n",
    "        entropy10_distances = []\n",
    "        test_entropy20 = [validation_labels[test_index][0], calculate_entropies(validation_images[test_index], num_frames=20)]\n",
    "        entropy20_distances = []\n",
    "        test_entropy30 = [validation_labels[test_index][0], calculate_entropies(validation_images[test_index], num_frames=30)]\n",
    "        entropy30_distances = []\n",
    "        test_entropy40 = [validation_labels[test_index][0], calculate_entropies(validation_images[test_index], num_frames=40)]\n",
    "        entropy40_distances = []\n",
    "        test_entropy50 = [validation_labels[test_index][0], calculate_entropies(validation_images[test_index], num_frames=50)]\n",
    "        entropy50_distances = []\n",
    "        test_entropy100 = [validation_labels[test_index][0], calculate_entropies(validation_images[test_index], num_frames=100)]\n",
    "        entropy100_distances = []\n",
    "    \n",
    "        test_entropy10x10 = [validation_labels[test_index][0], calculate_patch_entropies(validation_images[test_index][0], 10, 10)]\n",
    "        entropy10x10_distances = []\n",
    "        test_entropy20x20 = [validation_labels[test_index][0], calculate_patch_entropies(validation_images[test_index][0], 20, 20)]\n",
    "        entropy20x20_distances = []\n",
    "        test_entropy30x30 = [validation_labels[test_index][0], calculate_patch_entropies(validation_images[test_index][0], 30, 30)]\n",
    "        entropy30x30_distances = []\n",
    "        test_entropy40x40 = [validation_labels[test_index][0], calculate_patch_entropies(validation_images[test_index][0], 40, 40)]\n",
    "        entropy40x40_distances = []\n",
    "        test_entropy50x50 = [validation_labels[test_index][0], calculate_patch_entropies(validation_images[test_index][0], 50, 50)]\n",
    "        entropy50x50_distances = []\n",
    "        test_entropy70x70 = [validation_labels[test_index][0], calculate_patch_entropies(validation_images[test_index][0], 70, 70)]\n",
    "        entropy70x70_distances = []\n",
    "        test_entropy100x100 = [validation_labels[test_index][0], calculate_patch_entropies(validation_images[test_index][0], 100, 100)]\n",
    "        entropy100x100_distances = []\n",
    "        test_entropy100x200 = [validation_labels[test_index][0], calculate_patch_entropies(validation_images[test_index][0], 100, 200)]\n",
    "        entropy100x200_distances = []\n",
    "    \n",
    "    \n",
    "        for j in range(len(labels)):\n",
    "            if (current_id != id[j]):\n",
    "                entropy1_distances.append([entropy_list1[j][0], abs(entropy_list1[j][1][0] - test_entropy1[1][0])])\n",
    "                entropy10_distances.append([entropy_list10[j][0], mean_absolute_difference(entropy_list10[j][1], test_entropy10[1])])\n",
    "                entropy20_distances.append([entropy_list20[j][0], mean_absolute_difference(entropy_list20[j][1], test_entropy20[1])])\n",
    "                entropy30_distances.append([entropy_list30[j][0], mean_absolute_difference(entropy_list30[j][1], test_entropy30[1])])\n",
    "                entropy40_distances.append([entropy_list40[j][0], mean_absolute_difference(entropy_list40[j][1], test_entropy40[1])])\n",
    "                entropy50_distances.append([entropy_list50[j][0], mean_absolute_difference(entropy_list50[j][1], test_entropy50[1])])\n",
    "                entropy100_distances.append([entropy_list100[j][0], mean_absolute_difference(entropy_list100[j][1], test_entropy100[1])])\n",
    "    \n",
    "                entropy10x10_distances.append([entropy_list10x10[j][0], mean_absolute_difference(entropy_list10x10[j][1], test_entropy10x10[1])])\n",
    "                entropy20x20_distances.append([entropy_list20x20[j][0], mean_absolute_difference(entropy_list20x20[j][1], test_entropy20x20[1])])\n",
    "                entropy30x30_distances.append([entropy_list30x30[j][0], mean_absolute_difference(entropy_list30x30[j][1], test_entropy30x30[1])])\n",
    "                entropy40x40_distances.append([entropy_list40x40[j][0], mean_absolute_difference(entropy_list40x40[j][1], test_entropy40x40[1])])\n",
    "                entropy50x50_distances.append([entropy_list50x50[j][0], mean_absolute_difference(entropy_list50x50[j][1], test_entropy50x50[1])])\n",
    "                entropy70x70_distances.append([entropy_list70x70[j][0], mean_absolute_difference(entropy_list70x70[j][1], test_entropy70x70[1])])\n",
    "                entropy100x100_distances.append([entropy_list100x100[j][0], mean_absolute_difference(entropy_list100x100[j][1], test_entropy100x100[1])])\n",
    "                entropy100x200_distances.append([entropy_list100x200[j][0], mean_absolute_difference(entropy_list100x200[j][1], test_entropy100x200[1])])\n",
    "    \n",
    "    \n",
    "        # prediction for current test image\n",
    "        if (knncalc(k_knn, entropy1_distances) == 'genuine' and test_entropy1[0] == 'genuine') or (knncalc(k_knn, entropy1_distances) != 'genuine' and test_entropy1[0] != 'genuine'): correct_entropy1_preds += 1\n",
    "        if (knncalc(k_knn, entropy10_distances) == 'genuine' and test_entropy10[0] == 'genuine') or (knncalc(k_knn, entropy10_distances) != 'genuine' and test_entropy10[0] != 'genuine'): correct_entropy10_preds += 1\n",
    "        if (knncalc(k_knn, entropy20_distances) == 'genuine' and test_entropy20[0] == 'genuine') or (knncalc(k_knn, entropy20_distances) != 'genuine' and test_entropy20[0] != 'genuine'): correct_entropy20_preds += 1\n",
    "        if (knncalc(k_knn, entropy30_distances) == 'genuine' and test_entropy30[0] == 'genuine') or (knncalc(k_knn, entropy30_distances) != 'genuine' and test_entropy30[0] != 'genuine'): correct_entropy30_preds += 1\n",
    "        if (knncalc(k_knn, entropy40_distances) == 'genuine' and test_entropy40[0] == 'genuine') or (knncalc(k_knn, entropy40_distances) != 'genuine' and test_entropy40[0] != 'genuine'): correct_entropy40_preds += 1\n",
    "        if (knncalc(k_knn, entropy50_distances) == 'genuine' and test_entropy50[0] == 'genuine') or (knncalc(k_knn, entropy50_distances) != 'genuine' and test_entropy50[0] != 'genuine'): correct_entropy50_preds += 1\n",
    "        if (knncalc(k_knn, entropy100_distances) == 'genuine' and test_entropy100[0] == 'genuine') or (knncalc(k_knn, entropy100_distances) != 'genuine' and test_entropy100[0] != 'genuine'): correct_entropy100_preds += 1\n",
    "    \n",
    "        if (knncalc(k_knn, entropy10x10_distances) == 'genuine' and test_entropy10x10[0] == 'genuine') or (knncalc(k_knn, entropy10x10_distances) != 'genuine' and test_entropy10x10[0] != 'genuine'): correct_entropy10x10_preds += 1\n",
    "        if knncalc(k_knn, entropy20x20_distances) == 'genuine' and test_entropy20x20[0] == 'genuine' or knncalc(k_knn, entropy20x20_distances) != 'genuine' and test_entropy20x20[0] != 'genuine': correct_entropy20x20_preds += 1\n",
    "        if knncalc(k_knn, entropy30x30_distances) == 'genuine' and test_entropy30x30[0] == 'genuine' or knncalc(k_knn, entropy30x30_distances) != 'genuine' and test_entropy30x30[0] != 'genuine': correct_entropy30x30_preds += 1\n",
    "        if knncalc(k_knn, entropy40x40_distances) == 'genuine' and test_entropy40x40[0] == 'genuine' or knncalc(k_knn, entropy40x40_distances) != 'genuine' and test_entropy40x40[0] != 'genuine': correct_entropy40x40_preds += 1\n",
    "        if knncalc(k_knn, entropy50x50_distances) == 'genuine' and test_entropy50x50[0] == 'genuine' or knncalc(k_knn, entropy50x50_distances) != 'genuine' and test_entropy50x50[0] != 'genuine': correct_entropy50x50_preds += 1\n",
    "        if knncalc(k_knn, entropy70x70_distances) == 'genuine' and test_entropy70x70[0] == 'genuine' or knncalc(k_knn, entropy70x70_distances) != 'genuine' and test_entropy70x70[0] != 'genuine': correct_entropy70x70_preds += 1\n",
    "        if knncalc(k_knn, entropy100x100_distances) == 'genuine' and test_entropy100x100[0] == 'genuine' or knncalc(k_knn, entropy100x100_distances) != 'genuine' and test_entropy100x100[0] != 'genuine': correct_entropy100x100_preds += 1\n",
    "        if knncalc(k_knn, entropy100x200_distances) == 'genuine' and test_entropy100x200[0] == 'genuine' or knncalc(k_knn, entropy100x200_distances) != 'genuine' and test_entropy100x200[0] != 'genuine': correct_entropy100x200_preds += 1\n",
    "    \n",
    "        \n",
    "    # -----------------------------------------------------------------------\n",
    "    # OUTPUT RESULTS\n",
    "    # -----------------------------------------------------------------------\n",
    "    print(f'Leave One Subject Out')\n",
    "    total = len(validation_labels)\n",
    "    print(f'Total number of samples: {str(total)}')\n",
    "    print(f'Classification results \\nentropys with knn {k_knn}')\n",
    "    \n",
    "    print(f'Accuracy global entropy: {correct_entropy1_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 10 patches: {correct_entropy10_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 20 patches: {correct_entropy20_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 30 patches: {correct_entropy30_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 40 patches: {correct_entropy40_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 50 patches: {correct_entropy50_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 100 patches: {correct_entropy100_preds / total * 100:.3f}%')\n",
    "    \n",
    "    print(f'Accuracy entropy with 10x10 patches: {correct_entropy10x10_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 20x20 patches: {correct_entropy20x20_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 30x30 patches: {correct_entropy30x30_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 40x40 patches: {correct_entropy40x40_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 50x50 patches: {correct_entropy50x50_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 70x70 patches: {correct_entropy70x70_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 100x100 patches: {correct_entropy100x100_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy entropy with 100x200 patches: {correct_entropy100x200_preds / total * 100:.3f}%')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abd443c6e53d1da6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82dd109187743d7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# CALCULATE FEATURE VARIANCE\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# calculate feature variance\n",
    "variance_list1 = []\n",
    "variance_list10 = []\n",
    "variance_list20 = []\n",
    "variance_list30 = []\n",
    "variance_list40 = []\n",
    "variance_list50 = []\n",
    "variance_list100 = []\n",
    "\n",
    "#note: these patches divide image in vertical stripes\n",
    "for img in range(len(images)):\n",
    "    #global variance\n",
    "    variances1 = calculate_variances(images[img], 1)\n",
    "    variance_list1.append([labels[img], variances1])\n",
    "    #variance with 10 patches\n",
    "    variances10 = calculate_variances(images[img], 10)\n",
    "    variance_list10.append([labels[img], variances10])\n",
    "    #variance with 20 patches\n",
    "    variances20 = calculate_variances(images[img], 20)\n",
    "    variance_list20.append([labels[img], variances20])\n",
    "    #variance with 30 patches\n",
    "    variances30 = calculate_variances(images[img], 30)\n",
    "    variance_list30.append([labels[img], variances30])\n",
    "    #variance with 40 patches\n",
    "    variances40 = calculate_variances(images[img], 40)\n",
    "    variance_list40.append([labels[img], variances40])\n",
    "    #variance with 50 patches\n",
    "    variances50 = calculate_variances(images[img], 50)\n",
    "    variance_list50.append([labels[img], variances50])\n",
    "    #variance with 100 patches\n",
    "    variances100 = calculate_variances(images[img], 100)\n",
    "    variance_list100.append([labels[img], variances100])\n",
    "\n",
    "\n",
    "\n",
    "variance_list10x10 = []\n",
    "variance_list20x20 = []\n",
    "variance_list30x30 = []\n",
    "variance_list40x40 = []\n",
    "variance_list50x50 = []\n",
    "variance_list70x70 = []\n",
    "variance_list100x100 = []\n",
    "variance_list100x200 = []\n",
    "\n",
    "#note: these patches divide in a x b sized patches\n",
    "for img in range(len(images)):\n",
    "    #variance with 10x10 patches\n",
    "    variances10x10 = calculate_patch_variances(images[img], 10, 10)\n",
    "    variance_list10x10.append([labels[img], variances10x10])\n",
    "    #variance with 20x20 patches\n",
    "    variances20x20 = calculate_patch_variances(images[img], 20, 20)\n",
    "    variance_list20x20.append([labels[img], variances20x20])\n",
    "    #variance with 30x30 patches\n",
    "    variances30x30 = calculate_patch_variances(images[img], 30, 30)\n",
    "    variance_list30x30.append([labels[img], variances30x30])\n",
    "    #variance with 40x40 patches\n",
    "    variances40x40 = calculate_patch_variances(images[img], 40, 40)\n",
    "    variance_list40x40.append([labels[img], variances40x40])\n",
    "    #variance with 50x50 patches\n",
    "    variances50x50 = calculate_patch_variances(images[img], 50, 50)\n",
    "    variance_list50x50.append([labels[img], variances50x50])\n",
    "    # variance with 70x70 patches\n",
    "    variances70x70 = calculate_patch_variances(images[img], 70, 70)\n",
    "    variance_list70x70.append([labels[img], variances70x70])\n",
    "    #variance with 100x100 patches\n",
    "    variances100x100 = calculate_patch_variances(images[img], 100, 100)\n",
    "    variance_list100x100.append([labels[img], variances100x100])\n",
    "    #variance with 100x200 patches\n",
    "    variances100x200 = calculate_patch_variances(images[img], 100, 200)\n",
    "    variance_list100x200.append([labels[img], variances100x200])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c30d99f321f6799f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Leave one out"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd6272738a598531"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for k_knn in [3, 5, 7, 9]:\n",
    "    \n",
    "    correct_variance1_preds = 0\n",
    "    correct_variance10_preds = 0\n",
    "    correct_variance20_preds = 0\n",
    "    correct_variance30_preds = 0\n",
    "    correct_variance40_preds = 0\n",
    "    correct_variance50_preds = 0\n",
    "    correct_variance100_preds = 0\n",
    "    \n",
    "    correct_variance10x10_preds = 0\n",
    "    correct_variance20x20_preds = 0\n",
    "    correct_variance30x30_preds = 0\n",
    "    correct_variance40x40_preds = 0\n",
    "    correct_variance50x50_preds = 0\n",
    "    correct_variance70x70_preds = 0\n",
    "    correct_variance100x100_preds = 0\n",
    "    correct_variance100x200_preds = 0\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(loo.split(validation_images)):\n",
    "        current_id = validation_id[test_index]\n",
    "        \n",
    "        test_variance1 = [validation_labels[test_index][0], calculate_variances(validation_images[test_index], num_frames=1)]\n",
    "        variance1_distances = []\n",
    "        test_variance10 = [validation_labels[test_index][0], calculate_variances(validation_images[test_index][0], 10)]\n",
    "        variance10_distances = []\n",
    "        test_variance20 = [validation_labels[test_index][0], calculate_variances(validation_images[test_index][0], 20)]\n",
    "        variance20_distances = []\n",
    "        test_variance30 = [validation_labels[test_index][0], calculate_variances(validation_images[test_index][0], 30)]\n",
    "        variance30_distances = []\n",
    "        test_variance40 = [validation_labels[test_index][0], calculate_variances(validation_images[test_index][0], 40)]\n",
    "        variance40_distances = []\n",
    "        test_variance50 = [validation_labels[test_index][0], calculate_variances(validation_images[test_index][0], 50)]\n",
    "        variance50_distances = []\n",
    "        test_variance100 = [validation_labels[test_index][0], calculate_variances(validation_images[test_index][0], 100)]\n",
    "        variance100_distances = []\n",
    "    \n",
    "        test_variance10x10 = [validation_labels[test_index][0], calculate_patch_variances(validation_images[test_index][0], 10, 10)]\n",
    "        variance10x10_distances = []\n",
    "        test_variance20x20 = [validation_labels[test_index][0], calculate_patch_variances(validation_images[test_index][0], 20, 20)]\n",
    "        variance20x20_distances = []\n",
    "        test_variance30x30 = [validation_labels[test_index][0], calculate_patch_variances(validation_images[test_index][0], 30, 30)]\n",
    "        variance30x30_distances = []\n",
    "        test_variance40x40 = [validation_labels[test_index][0], calculate_patch_variances(validation_images[test_index][0], 40, 40)]\n",
    "        variance40x40_distances = []\n",
    "        test_variance50x50 = [validation_labels[test_index][0], calculate_patch_variances(validation_images[test_index][0], 50, 50)]\n",
    "        variance50x50_distances = []\n",
    "        test_variance70x70 = [validation_labels[test_index][0], calculate_patch_variances(validation_images[test_index][0], 70, 70)]\n",
    "        variance70x70_distances = []\n",
    "        test_variance100x100 = [validation_labels[test_index][0], calculate_patch_variances(validation_images[test_index][0], 100, 100)]\n",
    "        variance100x100_distances = []\n",
    "        test_variance100x200 = [validation_labels[test_index][0], calculate_patch_variances(validation_images[test_index][0], 100, 200)]\n",
    "        variance100x200_distances = []\n",
    "    \n",
    "    \n",
    "        for j in range(len(labels)):\n",
    "            variance1_distances.append([variance_list1[j][0], abs(variance_list1[j][1][0] - test_variance1[1][0])])\n",
    "            variance10_distances.append([variance_list10[j][0], mean_absolute_difference(variance_list10[j][1], test_variance10[1])])\n",
    "            variance20_distances.append([variance_list20[j][0], mean_absolute_difference(variance_list20[j][1], test_variance20[1])])\n",
    "            variance30_distances.append([variance_list30[j][0], mean_absolute_difference(variance_list30[j][1], test_variance30[1])])\n",
    "            variance40_distances.append([variance_list40[j][0], mean_absolute_difference(variance_list40[j][1], test_variance40[1])])\n",
    "            variance50_distances.append([variance_list50[j][0], mean_absolute_difference(variance_list50[j][1], test_variance50[1])])\n",
    "            variance100_distances.append([variance_list100[j][0], mean_absolute_difference(variance_list100[j][1], test_variance100[1])])\n",
    "                \n",
    "            variance10x10_distances.append([variance_list10x10[j][0], mean_absolute_difference(variance_list10x10[j][1], test_variance10x10[1])])\n",
    "            variance20x20_distances.append([variance_list20x20[j][0], mean_absolute_difference(variance_list20x20[j][1], test_variance20x20[1])])\n",
    "            variance30x30_distances.append([variance_list30x30[j][0], mean_absolute_difference(variance_list30x30[j][1], test_variance30x30[1])])\n",
    "            variance40x40_distances.append([variance_list40x40[j][0], mean_absolute_difference(variance_list40x40[j][1], test_variance40x40[1])])\n",
    "            variance50x50_distances.append([variance_list50x50[j][0], mean_absolute_difference(variance_list50x50[j][1], test_variance50x50[1])])\n",
    "            variance70x70_distances.append([variance_list70x70[j][0], mean_absolute_difference(variance_list70x70[j][1], test_variance70x70[1])])\n",
    "            variance100x100_distances.append([variance_list100x100[j][0], mean_absolute_difference(variance_list100x100[j][1], test_variance100x100[1])])\n",
    "            variance100x200_distances.append([variance_list100x200[j][0], mean_absolute_difference(variance_list100x200[j][1], test_variance100x200[1])])\n",
    "    \n",
    "    \n",
    "        # prediction for current test image\n",
    "        if (knncalc(k_knn, variance1_distances) == 'genuine' and test_variance1[0] == 'genuine') or (knncalc(k_knn, variance1_distances) != 'genuine' and test_variance1[0] != 'genuine'): correct_variance1_preds += 1\n",
    "        if (knncalc(k_knn, variance10_distances) == 'genuine' and test_variance10[0] == 'genuine') or (knncalc(k_knn, variance10_distances) != 'genuine' and test_variance10[0] != 'genuine'): correct_variance10_preds += 1\n",
    "        if knncalc(k_knn, variance20_distances) == 'genuine' and test_variance20[0] == 'genuine' or knncalc(k_knn, variance20_distances) != 'genuine' and test_variance20[0] != 'genuine': correct_variance20_preds += 1\n",
    "        if knncalc(k_knn, variance30_distances) == 'genuine' and test_variance30[0] == 'genuine' or knncalc(k_knn, variance30_distances) != 'genuine' and test_variance30[0] != 'genuine': correct_variance30_preds += 1\n",
    "        if knncalc(k_knn, variance40_distances) == 'genuine' and test_variance40[0] == 'genuine' or knncalc(k_knn, variance40_distances) != 'genuine' and test_variance40[0] != 'genuine': correct_variance40_preds += 1\n",
    "        if knncalc(k_knn, variance50_distances) == 'genuine' and test_variance50[0] == 'genuine' or knncalc(k_knn, variance50_distances) != 'genuine' and test_variance50[0] != 'genuine': correct_variance50_preds += 1\n",
    "        if knncalc(k_knn, variance100_distances) == 'genuine' and test_variance100[0] == 'genuine' or knncalc(k_knn, variance100_distances) != 'genuine' and test_variance100[0] != 'genuine': correct_variance100_preds += 1\n",
    "    \n",
    "        if (knncalc(k_knn, variance10x10_distances) == 'genuine' and test_variance10x10[0] == 'genuine') or (knncalc(k_knn, variance10x10_distances) != 'genuine' and test_variance10x10[0] != 'genuine'): correct_variance10x10_preds += 1\n",
    "        if knncalc(k_knn, variance20x20_distances) == 'genuine' and test_variance20x20[0] == 'genuine' or knncalc(k_knn, variance20x20_distances) != 'genuine' and test_variance20x20[0] != 'genuine': correct_variance20x20_preds += 1\n",
    "        if knncalc(k_knn, variance30x30_distances) == 'genuine' and test_variance30x30[0] == 'genuine' or knncalc(k_knn, variance30x30_distances) != 'genuine' and test_variance30x30[0] != 'genuine': correct_variance30x30_preds += 1\n",
    "        if knncalc(k_knn, variance40x40_distances) == 'genuine' and test_variance40x40[0] == 'genuine' or knncalc(k_knn, variance40x40_distances) != 'genuine' and test_variance40x40[0] != 'genuine': correct_variance40x40_preds += 1\n",
    "        if knncalc(k_knn, variance50x50_distances) == 'genuine' and test_variance50x50[0] == 'genuine' or knncalc(k_knn, variance50x50_distances) != 'genuine' and test_variance50x50[0] != 'genuine': correct_variance50x50_preds += 1\n",
    "        if knncalc(k_knn, variance70x70_distances) == 'genuine' and test_variance70x70[0] == 'genuine' or knncalc(k_knn, variance70x70_distances) != 'genuine' and test_variance70x70[0] != 'genuine': correct_variance70x70_preds += 1\n",
    "        if knncalc(k_knn, variance100x100_distances) == 'genuine' and test_variance100x100[0] == 'genuine' or knncalc(k_knn, variance100x100_distances) != 'genuine' and test_variance100x100[0] != 'genuine': correct_variance100x100_preds += 1\n",
    "        if knncalc(k_knn, variance100x200_distances) == 'genuine' and test_variance100x200[0] == 'genuine' or knncalc(k_knn, variance100x200_distances) != 'genuine' and test_variance100x200[0] != 'genuine': correct_variance100x200_preds += 1\n",
    "    \n",
    "        \n",
    "    # -----------------------------------------------------------------------\n",
    "    # OUTPUT RESULTS\n",
    "    # -----------------------------------------------------------------------\n",
    "    total = len(validation_labels)\n",
    "    print(f'Total number of samples: {str(total)}')\n",
    "    print(f'Classification results \\nVariances with knn {k_knn}')\n",
    "    \n",
    "    print(f'Accuracy global variance: {correct_variance1_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 10 patches: {correct_variance10_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 20 patches: {correct_variance20_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 30 patches: {correct_variance30_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 40 patches: {correct_variance40_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 50 patches: {correct_variance50_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 100 patches: {correct_variance100_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 10x10 patches: {correct_variance10x10_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 20x20 patches: {correct_variance20x20_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 30x30 patches: {correct_variance30x30_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 40x40 patches: {correct_variance40x40_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 50x50 patches: {correct_variance50x50_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 70x70 patches: {correct_variance70x70_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 100x100 patches: {correct_variance100x100_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 100x200 patches: {correct_variance100x200_preds / total * 100:.3f}%')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd5fc83ae4c196e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Leave one subject out\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca1a4591fdd0263b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for k_knn in [3, 5, 7, 9]:\n",
    "    \n",
    "    correct_variance1_preds = 0\n",
    "    correct_variance10_preds = 0\n",
    "    correct_variance20_preds = 0\n",
    "    correct_variance30_preds = 0\n",
    "    correct_variance40_preds = 0\n",
    "    correct_variance50_preds = 0\n",
    "    correct_variance100_preds = 0\n",
    "    correct_variance10x10_preds = 0\n",
    "    correct_variance20x20_preds = 0\n",
    "    correct_variance30x30_preds = 0\n",
    "    correct_variance40x40_preds = 0\n",
    "    correct_variance50x50_preds = 0\n",
    "    correct_variance70x70_preds = 0\n",
    "    correct_variance100x100_preds = 0\n",
    "    correct_variance100x200_preds = 0\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(loo.split(validation_images)):\n",
    "        current_id = validation_id[test_index]\n",
    "        \n",
    "        test_variance1 = [validation_labels[test_index][0], calculate_variances(validation_images[test_index], num_frames=1)]\n",
    "        variance1_distances = []\n",
    "        test_variance10 = [validation_labels[test_index][0], calculate_variances(validation_images[test_index], num_frames=10)]\n",
    "        variance10_distances = []\n",
    "        test_variance20 = [validation_labels[test_index][0], calculate_variances(validation_images[test_index], num_frames=20)]\n",
    "        variance20_distances = []\n",
    "        test_variance30 = [validation_labels[test_index][0], calculate_variances(validation_images[test_index], num_frames=30)]\n",
    "        variance30_distances = []\n",
    "        test_variance40 = [validation_labels[test_index][0], calculate_variances(validation_images[test_index], num_frames=40)]\n",
    "        variance40_distances = []\n",
    "        test_variance50 = [validation_labels[test_index][0], calculate_variances(validation_images[test_index], num_frames=50)]\n",
    "        variance50_distances = []\n",
    "        test_variance100 = [validation_labels[test_index][0], calculate_variances(validation_images[test_index], num_frames=100)]\n",
    "        variance100_distances = []\n",
    "    \n",
    "        test_variance10x10 = [validation_labels[test_index][0], calculate_patch_variances(validation_images[test_index][0], 10, 10)]\n",
    "        variance10x10_distances = []\n",
    "        test_variance20x20 = [validation_labels[test_index][0], calculate_patch_variances(validation_images[test_index][0], 20, 20)]\n",
    "        variance20x20_distances = []\n",
    "        test_variance30x30 = [validation_labels[test_index][0], calculate_patch_variances(validation_images[test_index][0], 30, 30)]\n",
    "        variance30x30_distances = []\n",
    "        test_variance40x40 = [validation_labels[test_index][0], calculate_patch_variances(validation_images[test_index][0], 40, 40)]\n",
    "        variance40x40_distances = []\n",
    "        test_variance50x50 = [validation_labels[test_index][0], calculate_patch_variances(validation_images[test_index][0], 50, 50)]\n",
    "        variance50x50_distances = []\n",
    "        test_variance70x70 = [validation_labels[test_index][0], calculate_patch_variances(validation_images[test_index][0], 70, 70)]\n",
    "        variance70x70_distances = []\n",
    "        test_variance100x100 = [validation_labels[test_index][0], calculate_patch_variances(validation_images[test_index][0], 100, 100)]\n",
    "        variance100x100_distances = []\n",
    "        test_variance100x200 = [validation_labels[test_index][0], calculate_patch_variances(validation_images[test_index][0], 100, 200)]\n",
    "        variance100x200_distances = []\n",
    "    \n",
    "    \n",
    "        for j in range(len(labels)):\n",
    "            if (current_id != id[j]):\n",
    "                variance1_distances.append([variance_list1[j][0], abs(variance_list1[j][1][0] - test_variance1[1][0])])\n",
    "                variance10_distances.append([variance_list10[j][0], mean_absolute_difference(variance_list10[j][1], test_variance10[1])])\n",
    "                variance20_distances.append([variance_list20[j][0], mean_absolute_difference(variance_list20[j][1], test_variance20[1])])\n",
    "                variance30_distances.append([variance_list30[j][0], mean_absolute_difference(variance_list30[j][1], test_variance30[1])])\n",
    "                variance40_distances.append([variance_list40[j][0], mean_absolute_difference(variance_list40[j][1], test_variance40[1])])\n",
    "                variance50_distances.append([variance_list50[j][0], mean_absolute_difference(variance_list50[j][1], test_variance50[1])])\n",
    "                variance100_distances.append([variance_list100[j][0], mean_absolute_difference(variance_list100[j][1], test_variance100[1])])\n",
    "    \n",
    "                variance10x10_distances.append([variance_list10x10[j][0], mean_absolute_difference(variance_list10x10[j][1], test_variance10x10[1])])\n",
    "                variance20x20_distances.append([variance_list20x20[j][0], mean_absolute_difference(variance_list20x20[j][1], test_variance20x20[1])])\n",
    "                variance30x30_distances.append([variance_list30x30[j][0], mean_absolute_difference(variance_list30x30[j][1], test_variance30x30[1])])\n",
    "                variance40x40_distances.append([variance_list40x40[j][0], mean_absolute_difference(variance_list40x40[j][1], test_variance40x40[1])])\n",
    "                variance50x50_distances.append([variance_list50x50[j][0], mean_absolute_difference(variance_list50x50[j][1], test_variance50x50[1])])\n",
    "                variance70x70_distances.append([variance_list70x70[j][0], mean_absolute_difference(variance_list70x70[j][1], test_variance70x70[1])])\n",
    "                variance100x100_distances.append([variance_list100x100[j][0], mean_absolute_difference(variance_list100x100[j][1], test_variance100x100[1])])\n",
    "                variance100x200_distances.append([variance_list100x200[j][0], mean_absolute_difference(variance_list100x200[j][1], test_variance100x200[1])])\n",
    "    \n",
    "    \n",
    "        # prediction for current test image\n",
    "        if (knncalc(k_knn, variance1_distances) == 'genuine' and test_variance1[0] == 'genuine') or (knncalc(k_knn, variance1_distances) != 'genuine' and test_variance1[0] != 'genuine'): correct_variance1_preds += 1\n",
    "        if (knncalc(k_knn, variance10_distances) == 'genuine' and test_variance10[0] == 'genuine') or (knncalc(k_knn, variance10_distances) != 'genuine' and test_variance10[0] != 'genuine'): correct_variance10_preds += 1\n",
    "        if (knncalc(k_knn, variance20_distances) == 'genuine' and test_variance20[0] == 'genuine') or (knncalc(k_knn, variance20_distances) != 'genuine' and test_variance20[0] != 'genuine'): correct_variance20_preds += 1\n",
    "        if (knncalc(k_knn, variance30_distances) == 'genuine' and test_variance30[0] == 'genuine') or (knncalc(k_knn, variance30_distances) != 'genuine' and test_variance30[0] != 'genuine'): correct_variance30_preds += 1\n",
    "        if (knncalc(k_knn, variance40_distances) == 'genuine' and test_variance40[0] == 'genuine') or (knncalc(k_knn, variance40_distances) != 'genuine' and test_variance40[0] != 'genuine'): correct_variance40_preds += 1\n",
    "        if (knncalc(k_knn, variance50_distances) == 'genuine' and test_variance50[0] == 'genuine') or (knncalc(k_knn, variance50_distances) != 'genuine' and test_variance50[0] != 'genuine'): correct_variance50_preds += 1\n",
    "        if (knncalc(k_knn, variance100_distances) == 'genuine' and test_variance100[0] == 'genuine') or (knncalc(k_knn, variance100_distances) != 'genuine' and test_variance100[0] != 'genuine'): correct_variance100_preds += 1\n",
    "    \n",
    "        if (knncalc(k_knn, variance10x10_distances) == 'genuine' and test_variance10x10[0] == 'genuine') or (knncalc(k_knn, variance10x10_distances) != 'genuine' and test_variance10x10[0] != 'genuine'): correct_variance10x10_preds += 1\n",
    "        if knncalc(k_knn, variance20x20_distances) == 'genuine' and test_variance20x20[0] == 'genuine' or knncalc(k_knn, variance20x20_distances) != 'genuine' and test_variance20x20[0] != 'genuine': correct_variance20x20_preds += 1\n",
    "        if knncalc(k_knn, variance30x30_distances) == 'genuine' and test_variance30x30[0] == 'genuine' or knncalc(k_knn, variance30x30_distances) != 'genuine' and test_variance30x30[0] != 'genuine': correct_variance30x30_preds += 1\n",
    "        if knncalc(k_knn, variance40x40_distances) == 'genuine' and test_variance40x40[0] == 'genuine' or knncalc(k_knn, variance40x40_distances) != 'genuine' and test_variance40x40[0] != 'genuine': correct_variance40x40_preds += 1\n",
    "        if knncalc(k_knn, variance50x50_distances) == 'genuine' and test_variance50x50[0] == 'genuine' or knncalc(k_knn, variance50x50_distances) != 'genuine' and test_variance50x50[0] != 'genuine': correct_variance50x50_preds += 1\n",
    "        if knncalc(k_knn, variance70x70_distances) == 'genuine' and test_variance70x70[0] == 'genuine' or knncalc(k_knn, variance70x70_distances) != 'genuine' and test_variance70x70[0] != 'genuine': correct_variance70x70_preds += 1\n",
    "        if knncalc(k_knn, variance100x100_distances) == 'genuine' and test_variance100x100[0] == 'genuine' or knncalc(k_knn, variance100x100_distances) != 'genuine' and test_variance100x100[0] != 'genuine': correct_variance100x100_preds += 1\n",
    "        if knncalc(k_knn, variance100x200_distances) == 'genuine' and test_variance100x200[0] == 'genuine' or knncalc(k_knn, variance100x200_distances) != 'genuine' and test_variance100x200[0] != 'genuine': correct_variance100x200_preds += 1\n",
    "    \n",
    "        \n",
    "    # -----------------------------------------------------------------------\n",
    "    # OUTPUT RESULTS\n",
    "    # -----------------------------------------------------------------------\n",
    "    total = len(validation_labels)\n",
    "    print(f'Total number of samples: {str(total)}')\n",
    "    print(f'Classification results \\nVariances with knn {k_knn}')\n",
    "    \n",
    "    print(f'Accuracy global variance: {correct_variance1_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 10 patches: {correct_variance10_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 20 patches: {correct_variance20_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 30 patches: {correct_variance30_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 40 patches: {correct_variance40_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 50 patches: {correct_variance50_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 100 patches: {correct_variance100_preds / total * 100:.3f}%')\n",
    "    \n",
    "    print(f'Accuracy variance with 10x10 patches: {correct_variance10x10_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 20x20 patches: {correct_variance20x20_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 30x30 patches: {correct_variance30x30_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 40x40 patches: {correct_variance40x40_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 50x50 patches: {correct_variance50x50_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 70x70 patches: {correct_variance70x70_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 100x100 patches: {correct_variance100x100_preds / total * 100:.3f}%')\n",
    "    print(f'Accuracy variance with 100x200 patches: {correct_variance100x200_preds / total * 100:.3f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "421e1ba14ac02bc6"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dcdf63c55e8ca230"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Histogram\n",
    "\n",
    "Histogram has no prior feature calculation as the histograms were calculated in the import method due to computational advantages.\n",
    "The downside is, that the amount of bins need to be determined before the data import. (todo: try out different bins)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8411aebb6972614"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correct_em_preds = 0\n",
    "correct_it_preds = 0\n",
    "correct_ed_preds = 0\n",
    "correct_smd_preds = 0\n",
    "correct_hist_combined_preds = 0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10c83f60f31b0e8f"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ff8cbf680c4c543"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loo\n",
    "\n",
    "\"\n",
    "def cross val\n",
    "    loo\n",
    "        train data  = [train index]\n",
    "        test data = [test index]\n",
    "\n",
    "        knn. fit(trainx, train y)\n",
    "        knn. predict (testx, testy)\n",
    "\n",
    "        return accuracy score\n",
    "\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d4118ced18d6c14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for k_knn in [3, 5, 7, 9]:\n",
    "    for i, (train_index, test_index) in enumerate(loo.split(validation_images)):\n",
    "        current_id = validation_id[test_index]\n",
    "       \n",
    "        # -----------------------------------------------------------------------\n",
    "        # HISTOGRAM\n",
    "        # -----------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "        # intersection distance (runs fast)\n",
    "        it_distance = []\n",
    "        for j in range(len(labels)):\n",
    "            if (current_id != id[j]):\n",
    "                it = intersection_test(histograms[j], validation_histograms[test_index])\n",
    "                it_distance.append([labels[j], it])\n",
    "    \n",
    "        # euclidian distance\n",
    "        ed_distance = []\n",
    "        for j in range(len(labels)):\n",
    "            if (current_id != id[j]):\n",
    "                ed = euclidean_distance_test(histograms[j], validation_histograms[test_index])\n",
    "                ed_distance.append([labels[j], ed])\n",
    "    \n",
    "        # sum of manhattan distances\n",
    "        smd_distance = []\n",
    "        for j in range(len(labels)):\n",
    "            if (current_id != id[j]):\n",
    "                smd = sum_manhattan_distance_test(histograms[j], validation_histograms[test_index])\n",
    "                smd_distance.append([labels[j], smd])\n",
    "    \n",
    "        # prediction for current test image\n",
    "        k_knn = 3\n",
    "        # pred_em = knncalc(k_knn, em_distance)\n",
    "        pred_it = knncalc(k_knn, it_distance)\n",
    "        pred_ed = knncalc(k_knn, ed_distance)\n",
    "        pred_smd = knncalc(k_knn, smd_distance)\n",
    "    \n",
    "        # pred_hist_combined = []\n",
    "        # pred_hist_combined.append([pred_it, pred_ed, pred_smd])\n",
    "        # pred_hist_combined_value = most_common_value(pred_hist_combined)\n",
    "    \n",
    "        if pred_it == 'genuine' and validation_labels[test_index][0] == 'genuine' or pred_it == 'synthethic' and \\\n",
    "                validation_labels[test_index][0] != 'genuine':\n",
    "            correct_it_preds += 1\n",
    "    \n",
    "        if pred_ed == 'genuine' and validation_labels[test_index][0] == 'genuine' or pred_ed == 'synthethic' and \\\n",
    "                validation_labels[test_index][0] != 'genuine':\n",
    "            correct_ed_preds += 1\n",
    "    \n",
    "        if pred_smd == 'genuine' and validation_labels[test_index][0] == 'genuine' or pred_smd == 'synthethic' and \\\n",
    "                validation_labels[test_index][0] != 'genuine':\n",
    "            correct_smd_preds += 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50f9c556cbd1f48f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Output results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84cec059c2b3dc5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total = len(validation_labels)\n",
    "print(f'Total number of samples: {str(total)}')\n",
    "\n",
    "accuracy_em = correct_em_preds / total\n",
    "print(f'Accuracy em: {accuracy_em * 100:.2f}%')\n",
    "\n",
    "accuracy_it = correct_it_preds / total\n",
    "print(f'Accuracy it: {accuracy_it * 100:.2f}%')\n",
    "\n",
    "accuracy_ed = correct_ed_preds / total\n",
    "print(f'Accuracy ed: {accuracy_ed * 100:.2f}%')\n",
    "\n",
    "accuracy_smd = correct_smd_preds / total\n",
    "print(f'Accuracy smd: {accuracy_smd * 100:.2f}%')\n",
    "\n",
    "accuracy_hist_combined = correct_hist_combined_preds / total\n",
    "print(f\"Accuracy combined of hist: {accuracy_hist_combined*100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d25f280a52d5264d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
