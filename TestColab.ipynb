{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "from skimage.measure import shannon_entropy\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# DATA IMPORT\n",
    "# -----------------------------------------------------------------------\n",
    "def loadPLUS():\n",
    "    # Define the path to the dataset\n",
    "    datasource_path = \"dataset/PLUS\"\n",
    "\n",
    "    data_PLUS_genuine, data_PLUS_spoofed, data_PLUS_003, data_PLUS_004 = [], [], [], []\n",
    "\n",
    "    # load dataset PLUS\n",
    "    p = Path(datasource_path + \"/\" + \"genuine\")\n",
    "    for filename in p.glob('**/*.png'):\n",
    "        img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "        hist = histBin(img, k)\n",
    "        data_PLUS_genuine.append([\"genuine\", img, hist])\n",
    "\n",
    "    \"\"\"p = Path(datasource_path + \"/\" + \"spoofed\")\n",
    "    for filename in p.glob('**/*.png'):\n",
    "        img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "        histBin(img, k)\n",
    "        data_PLUS_spoofed.append([\"spoofed\", img, hist])\"\"\"\n",
    "\n",
    "    for synthethic_category in [\"spoofed_synthethic_cyclegan\",\n",
    "                                \"spoofed_synthethic_distancegan\",\n",
    "                                \"spoofed_synthethic_stargan-v2\"]:\n",
    "        for variant in [\"003\"]:\n",
    "            for fold in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n",
    "                p = Path(datasource_path + \"/\" + synthethic_category + \"/\" + variant + \"/\" + fold)\n",
    "                for filename in p.glob('**/*.png'):\n",
    "                    img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "                    hist = histBin(img, k)\n",
    "                    data_PLUS_003.append([\"synthethic\", img, hist])\n",
    "        \"\"\"for variant in [\"004\"]:\n",
    "            for fold in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n",
    "                p = Path(datasource_path + \"/\" + synthethic_category + \"/\" + variant + \"/\" + fold)\n",
    "                for filename in p.glob('**/*.png'):\n",
    "                    img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "                    hist = histBin(img, k)\n",
    "                    data_PLUS_004.append([\"synthethic\", img, hist])\"\"\"\n",
    "\n",
    "    return data_PLUS_genuine, data_PLUS_spoofed, data_PLUS_003, data_PLUS_004\n",
    "\n",
    "\n",
    "\n",
    "def loadSCUT():\n",
    "    # Define the path to the dataset\n",
    "    datasource_path = \"dataset/SCUT\"\n",
    "\n",
    "    data_SCUT_genuine, data_SCUT_spoofed, data_SCUT_007, data_SCUT_008 = [], [], [], []\n",
    "\n",
    "    # load dataset PLUS\n",
    "    p = Path(datasource_path + \"/\" + \"genuine\")\n",
    "    for filename in p.glob('**/*.png'):\n",
    "        img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "        hist = histBin(img, k)\n",
    "        data_SCUT_genuine.append([\"genuine\", img, hist])\n",
    "\n",
    "    p = Path(datasource_path + \"/\" + \"spoofed\")\n",
    "    for filename in p.glob('**/*.png'):\n",
    "        img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "        hist = hist = histBin(img, k)\n",
    "        data_SCUT_spoofed.append([\"spoofed\", img, hist])\n",
    "\n",
    "    for synthethic_category in [\"spoofed_synthethic_cyclegan\",\n",
    "                                \"spoofed_synthethic_distancegan\",      # ignore \"spoofed_synthethic_drit\",\n",
    "                                \"spoofed_synthethic_stargan-v2\"]:\n",
    "        for variant in [\"007\"]:\n",
    "            for fold in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n",
    "                p = Path(datasource_path + \"/\" + synthethic_category + \"/\" + variant + \"/\" + fold)\n",
    "                for filename in p.glob('**/*.png'):\n",
    "                    img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "                    hist = histBin(img, k)\n",
    "                    data_SCUT_007.append([\"synthethic\", img, hist])\n",
    "        for variant in [\"008\"]:\n",
    "            for fold in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n",
    "                p = Path(datasource_path + \"/\" + synthethic_category + \"/\" + variant + \"/\" + fold)\n",
    "                for filename in p.glob('**/*.png'):\n",
    "                    img = cv2.imread(str(filename), cv2.IMREAD_GRAYSCALE)\n",
    "                    hist = histBin(img, k)\n",
    "                    data_SCUT_008.append([\"synthethic\", img, hist])\n",
    "\n",
    "    return data_SCUT_genuine, data_SCUT_spoofed, data_SCUT_007, data_SCUT_008\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# KNN\n",
    "# -----------------------------------------------------------------------\n",
    "def knncalc(k, feature_list):\n",
    "    knn_list = list()\n",
    "    feature_list.sort(key=lambda tup: tup[1]) #nach den Werten in der zweiten Spalte sortieren (kng)\n",
    "    for i in range(k):\n",
    "        knn_list.append(feature_list[i][0]) # es werden \"spoofed\" und \"genuine\" eingetragen\n",
    "    fdist = dict(zip(*np.unique(knn_list, return_counts=True)))\n",
    "    pred = max(fdist, key=fdist.get)\n",
    "    return pred\n",
    "\n",
    "\n",
    "####### for last step if we have several feature classifications\n",
    "def most_common_value(list):\n",
    "    mcv = dict(zip(*np.unique(list, return_counts=True)))\n",
    "    pred = list(mcv)[-1]\n",
    "    return pred\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# HISTOGRAM\n",
    "# -----------------------------------------------------------------------\n",
    "k = 2\n",
    "def histBin(img, k):\n",
    "    if k<257:\n",
    "        hist, bin_edges1 = np.histogram(img, bins=np.arange(0, k), density=True)\n",
    "        return hist\n",
    "    else:\n",
    "        return \"Falscher k-Wert\"\n",
    "\n",
    "def intersection_test_(hist1, hist2):\n",
    "    intersection = np.minimum(hist1, hist2)\n",
    "    int_area = intersection.sum() # /2 wenn die bins so aussehen bins=np.arange(0, 256, 0.5)\n",
    "    resultat = 1-int_area\n",
    "    return resultat\n",
    "\n",
    "def euclidean_distance_test(hist1, hist2):\n",
    "    sum = 0\n",
    "    for i in range (0,256):\n",
    "        sum = sum + (hist1[i][0]-hist2[i][0])**2\n",
    "    eu_dist = math.sqrt(sum)\n",
    "    return eu_dist\n",
    "\n",
    "def manhattan_distance_funktion(x, y):\n",
    "    return (abs(x-y))\n",
    "def sum_manhattan_distance_test(hist1, hist2):\n",
    "    sum = 0\n",
    "    for i in range (0,256):\n",
    "        sum = sum + manhattan_distance_funktion(hist1[i][0], hist2[i][0])\n",
    "    ma_dist = sum\n",
    "    return ma_dist\n",
    "\n",
    "#earthmovers distance\n",
    "def em_dist(img1, img2):\n",
    "    images = [img.ravel() for img in [img1, img2]]\n",
    "    em_distance = wasserstein_distance(images[0], images[1])\n",
    "    return em_distance\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# ENTROPY\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# calculate Entropy with frames--> takes nr of frames and image path and gives back an array of entropies:\n",
    "def calculate_entropies(image, num_frames):\n",
    "\n",
    "    #image = image[1]\n",
    "    # Divide the image into frames\n",
    "    entropies = []\n",
    "    size = image.shape\n",
    "    height, width = size[0], size[1]\n",
    "\n",
    "    frame_size = width // num_frames\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        start_x = i * frame_size\n",
    "        end_x = (i + 1) * frame_size\n",
    "        frame = image[:, start_x:end_x]\n",
    "        entropy = shannon_entropy(frame)\n",
    "        entropies.append(entropy)\n",
    "\n",
    "    return entropies\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# VARIANCE\n",
    "# -----------------------------------------------------------------------\n",
    "def calculate_variances(image, num_frames):\n",
    "\n",
    "    image = image[1]\n",
    "    # Divide the image into frames\n",
    "    variances = []\n",
    "    height, width = image.shape\n",
    "\n",
    "    frame_size = width // num_frames\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        start_x = i * frame_size\n",
    "        end_x = (i + 1) * frame_size\n",
    "        frame = image[:, start_x:end_x]\n",
    "        variance = np.var(frame)\n",
    "        variances.append(variance)\n",
    "\n",
    "    return variances\n",
    "\n",
    "# Function to calculate patch variances of an image with size patch_x x patch_y\n",
    "def calculate_patch_variances(image, patch_x, patch_y):\n",
    "    patch_size = (patch_x, patch_y)\n",
    "    variances = []\n",
    "    h, w = image.shape\n",
    "    for i in range(0, h, patch_size[0]):\n",
    "        for j in range(0, w, patch_size[1]):\n",
    "            patch = image[i:i+patch_size[0], j:j+patch_size[1]]\n",
    "            variances.append(np.var(patch))\n",
    "    return variances\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# METHODEN-AUFRUFE\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "data_PLUS_genuine, data_PLUS_spoofed, data_PLUS_003, data_PLUS_004 = loadPLUS()\n",
    "#data_SCUT_genuine, data_SCUT_spoofed, data_SCUT_007, data_SCUT_008 = loadSCUT()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# LEAVE ONE OUT CROSS VALIDATION\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "def combine_list_with_genuine(list):\n",
    "    current_data = data_PLUS_genuine + list\n",
    "    return current_data\n",
    "\n",
    "# combine lists data_PLUS_genuine and data_PLUS_003\n",
    "current_data = []\n",
    "current_data = combine_list_with_genuine(data_PLUS_003)\n",
    "\n",
    "\n",
    "# convert data to numpy array\n",
    "labels_list, images_list, histograms_list = [], [], []\n",
    "counter = 0\n",
    "for row in current_data:\n",
    "    labels_list.append(row[0])\n",
    "    images_list.append(row[1])\n",
    "    histograms_list.append(row[2])\n",
    "    #counter += 1       #for debugging purposes\n",
    "features = [cv2.resize(img, (736,192)) for img in images_list]      #Alternative sklearn.preprocessing.StandardScaler\n",
    "labels = np.array(labels_list)\n",
    "images = np.array(features)\n",
    "histograms = np.array(histograms_list)\n",
    "#print(\"counter= \" + str(counter))          #for debugging purposes\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# ENTROPY\n",
    "# -----------------------------------------------------------------------\n",
    "# calculate feature global entropy\n",
    "num_frames = 1\n",
    "entropy_list = []\n",
    "for img in range(len(images)):\n",
    "    entropies = calculate_entropies(images[img], num_frames)\n",
    "    entropy_list.append([labels[img], entropies])\n",
    "\n",
    "\n",
    "\n",
    "# knn global entropy\n",
    "\n",
    "\n",
    "# calculate feature patch entropy\n",
    "\n",
    "# knn patch entropy\n",
    "\n",
    "\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "pred_list = []\n",
    "correct_entropy_preds = 0\n",
    "correct_em_preds = 0\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(loo.split(images)):\n",
    "    #calculate distances\n",
    "    #train, test = entropy_list[train_index], entropy_list[test_index]\n",
    "    test_entropy = entropy_list[test_index[0]]\n",
    "    entropy_distances = []\n",
    "    for j in train_index:\n",
    "        entropy_distances.append([entropy_list[j][0], abs(entropy_list[j][1][0] - test_entropy[1][0])])\n",
    "\n",
    "    #prediction for current test image\n",
    "    k_knn = 3\n",
    "    pred_entropy = knncalc(k_knn, entropy_distances)\n",
    "\n",
    "    if pred_entropy == test_entropy[0]:\n",
    "        pred_list.append(0)\n",
    "        correct_entropy_preds += 1\n",
    "    else:\n",
    "        pred_list.append(1)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # VARIANCE\n",
    "    # -----------------------------------------------------------------------\n",
    "    # calculate feature global variance\n",
    "\n",
    "    # knn global variance\n",
    "\n",
    "    # calculate feature patch variance\n",
    "\n",
    "    # knn patch variance\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # HISTOGRAM\n",
    "    # -----------------------------------------------------------------------\n",
    "    # calculate feature histogram\n",
    "    # calculate feature global entropy\n",
    "\n",
    "    #earthmovers distance\n",
    "    em_distance = []\n",
    "    for j in range(len(images)):\n",
    "        em = em_dist(images[test_index], images[j])\n",
    "        em_distance.append([labels[j], em])\n",
    "\n",
    "    # prediction for current test image\n",
    "    k_knn = 3\n",
    "    pred_em = knncalc(k_knn, em_distance)\n",
    "\n",
    "    if pred_em == labels[test_index]:\n",
    "        correct_em_preds += 1\n",
    "\n",
    "\n",
    "\n",
    "accuracy_entropy = correct_entropy_preds / len(images)\n",
    "print(f\"Accuracy entropy with Leave-One-Out Cross-Validation: {accuracy_entropy*100:.2f}%\")\n",
    "\n",
    "accuracy_em = correct_em_preds / len(images)\n",
    "print(f\"Accuracy em with Leave-One-Out Cross-Validation: {accuracy_em*100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1116d8bab71d800c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
